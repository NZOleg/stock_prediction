{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Olegok\\Anaconda3\\envs\\py36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas import datetime\n",
    "import math, time\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Bidirectional\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing import sequence\n",
    "import csv \n",
    "import os.path\n",
    "import gc\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(seq_len = 20):\n",
    "    data = pd.read_csv('stocks/AAPL.csv')\n",
    "    data = data.drop(['direction_up', 'direction_down','Date', 'direction'], axis=1)\n",
    "    data = data.dropna(axis=0, how='any')\n",
    "    min_max_scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "    for key in data:\n",
    "        data[key] = min_max_scaler.fit_transform(data[key].values.reshape(-1,1))\n",
    "    x_data = data.as_matrix()\n",
    "    x_data_seq = []\n",
    "    y_data_seq = []\n",
    "    for index in range(len(data) - seq_len): \n",
    "        x_data_seq.append(x_data[index: index + seq_len])\n",
    "        y_data_seq.append(x_data[index + seq_len])\n",
    "    x_train = np.array(x_data_seq[:math.ceil(len(data)*0.8)][:][:])\n",
    "    y_train = np.array(y_data_seq[:math.ceil(len(data)*0.8)][:])\n",
    "    x_test = np.array(x_data_seq[math.ceil(len(data)*0.8):][:][:])\n",
    "    y_test = np.array(y_data_seq[math.ceil(len(data)*0.8):][:])\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model(_batch_size, _n_neurons=128 , _dropout=0.5, seq_len=20, _activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(_n_neurons, batch_input_shape=(_batch_size, seq_len, 36)))\n",
    "    model.add(Dropout(_dropout))\n",
    "    #model.add(Flatten())\n",
    "    model.add(Dense(36, activation=_activation)) #relu & signoid\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_dataset(batch_size, x_train, y_train, x_test, y_test): \n",
    "    x_train = x_train[:(x_train.shape[0]//batch_size)*batch_size]\n",
    "    y_train = y_train[:(y_train.shape[0]//batch_size)*batch_size]\n",
    "    x_test = x_test[:(x_test.shape[0]//batch_size)*batch_size]\n",
    "    y_test = y_test[:(y_test.shape[0]//batch_size)*batch_size]\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7488 samples, validate on 1856 samples\n",
      "Epoch 1/1\n",
      "7488/7488 [==============================] - 5s 625us/step - loss: 0.0446 - val_loss: 0.0562\n",
      "1856/1856 [==============================] - 0s 139us/step\n",
      "Train on 7488 samples, validate on 1856 samples\n",
      "Epoch 1/1\n",
      "7488/7488 [==============================] - 5s 669us/step - loss: 0.0396 - val_loss: 0.0505\n",
      "1856/1856 [==============================] - 0s 156us/step\n",
      "Train on 7488 samples, validate on 1856 samples\n",
      "Epoch 1/1\n",
      "7488/7488 [==============================] - 5s 677us/step - loss: 0.0378 - val_loss: 0.0505\n",
      "1856/1856 [==============================] - 0s 155us/step\n",
      "Train on 7488 samples, validate on 1856 samples\n",
      "Epoch 1/1\n",
      "7488/7488 [==============================] - 6s 779us/step - loss: 0.0404 - val_loss: 0.0506\n",
      "1856/1856 [==============================] - 0s 188us/step\n",
      "Train on 7488 samples, validate on 1856 samples\n",
      "Epoch 1/1\n",
      "7488/7488 [==============================] - 6s 861us/step - loss: 0.0396 - val_loss: 0.0490\n",
      "1856/1856 [==============================] - 0s 182us/step\n",
      "Train on 7488 samples, validate on 1856 samples\n",
      "Epoch 1/1\n",
      "7488/7488 [==============================] - 6s 808us/step - loss: 0.0356 - val_loss: 0.0407\n",
      "1856/1856 [==============================] - 0s 212us/step\n",
      "Train on 7488 samples, validate on 1856 samples\n",
      "Epoch 1/1\n",
      "7488/7488 [==============================] - 7s 950us/step - loss: 0.0381 - val_loss: 0.0481\n",
      "1856/1856 [==============================] - 1s 313us/step\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'tryin-2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-0a6fdc6513c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_neurons\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m#writer.writerow(['Time', 'Score', 'n_neurons'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'tryin-2.csv'"
     ]
    }
   ],
   "source": [
    "n_epoch=25\n",
    "file_name = r'neurons_tune.csv'\n",
    "if (os.path.exists(file_name)):\n",
    "    logs = pd.read_csv(file_name).tail(1).as_matrix()\n",
    "    pointer = logs[0,2] \n",
    "else:\n",
    "    pointer = 0\n",
    "for n_neurons in range(20,500,10):\n",
    "    batch = 10\n",
    "    if pointer >= n_neurons:\n",
    "        continue\n",
    "    data = []\n",
    "    start_time = time.time()\n",
    "    model = Model(batch,n_neurons)\n",
    "    x_train, y_train, x_test, y_test = create_dataset()\n",
    "    x_train, y_train, x_test, y_test = fit_dataset(batch, x_train, y_train, x_test, y_test)\n",
    "    model.fit(x_train,\n",
    "         y_train,\n",
    "        batch_size=batch,\n",
    "         epochs=n_epoch,\n",
    "       validation_data=(x_test, y_test),\n",
    "        verbose=1)\n",
    "    score = model.evaluate(x_test, y_test, batch_size=batch)\n",
    "    data.append(score)\n",
    "    data.append(time.time() - start_time)\n",
    "    data.append(n_neurons)\n",
    "    del model\n",
    "    with open(file_name, 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        #writer.writerow(['Time', 'Score', 'n_neurons'])\n",
    "        writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
